{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer-    Information Gain is a metric used in decision trees to decide which feature to split on at each node. It measures how much uncertainty (impurity) in the data is reduced after making a split.\n",
        "\n",
        "1. Core Idea\n",
        "\n",
        "In decision trees, the goal is to split the data so that the resulting subsets are as pure as possible (i.e., contain mostly one class).\n",
        "\n",
        "-   Higher Information Gain ‚Üí Better split.\n",
        "\n",
        "-   It answers:\n",
        "\n",
        "    ‚ÄúWhich feature gives us the most information about the target variable?‚Äù\n",
        "\n",
        "2. Entropy (Measure of Uncertainty)\n",
        "Information Gain is based on Entropy, which quantifies randomness or impurity.\n",
        "\n",
        "Entropy Formula:\n",
        "\n",
        "     Entropy(S)=‚àíi=1‚àëc‚Äãpi‚Äãlog2‚Äã(pi‚Äã)\n",
        "\n",
        "Where:\n",
        "\n",
        "-   ùëùi= proportion of class ùëñ\n",
        "\n",
        "-   c = number of classes\n",
        "\n",
        "Key Points:\n",
        "\n",
        "-   Entropy = 0 ‚Üí completely pure (all one class)\n",
        "\n",
        "-   Entropy = 1 (binary case) ‚Üí maximum uncertainty\n",
        "\n",
        "\n",
        "\n",
        "3. Information Gain Formula\n",
        "\n",
        "       Information Gain(S,A)=Entropy(S)‚àív‚ààA‚àë‚Äã‚à£S‚à£‚à£Sv‚Äã‚à£‚ÄãEntropy(Sv‚Äã)\n",
        "Where:\n",
        "\n",
        "\n",
        "-   S = original dataset\n",
        "\n",
        "-   A = feature being split\n",
        "\n",
        "-   Sv‚Äã = subset of S where feature A has value v\n",
        "\n",
        "4. How It Is Used in Decision Trees\n",
        "\n",
        "1.Compute entropy of the parent node\n",
        "\n",
        "2.For each candidate feature:\n",
        "\n",
        "-   Split the data based on feature values\n",
        "\n",
        "-   Compute entropy of each subset\n",
        "\n",
        "-   Calculate Information Gain\n",
        "\n",
        "3.Choose the feature with the highest Information Gain\n",
        "\n",
        "4.Repeat recursively for child nodes until:\n",
        "\n",
        "-   Data is pure\n",
        "\n",
        "-   No features remain\n",
        "\n",
        "-   Stopping criteria is met\n",
        "\n",
        "   5. Simple Example\n",
        "Suppose a dataset has:\n",
        "\n",
        "-   9 ‚ÄúYes‚Äù\n",
        "\n",
        "-   5 ‚ÄúNo‚Äù\n",
        "\n",
        "Initial entropy ‚âà 0.94\n",
        "\n",
        "If a feature split reduces entropy to 0.5, then:\n",
        "\n",
        "Information Gain=0.94‚àí0.5=0.44\n",
        "\n",
        "This feature is a good candidate for splitting.\n",
        "\n",
        "6. Advantages and Limitations\n",
        "\n",
        " Advantages\n",
        "\n",
        "-    Intuitive and mathematically grounded\n",
        "\n",
        "-    Works well for classification tasks\n",
        "\n",
        " Limitations\n",
        "\n",
        "-   Biased toward features with many unique values\n",
        "\n",
        "-   This issue is addressed by Gain Ratio (used in C4.5)\n",
        "\n",
        "7. Summary\n",
        "\n",
        "-   Information Gain measures the reduction in entropy after a split\n",
        "\n",
        "-    Used to select the best feature at each node\n",
        "\n",
        "-    A core concept in ID3 and C4.5 decision tree algorithms\n",
        "\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n",
        "\n",
        "Answer-  Gini Impurity and Entropy are the two most common measures used to quantify impurity in decision trees. Both aim to evaluate how mixed the classes are in a dataset, but they differ in calculation, interpretation, and practical behavior.\n",
        "\n",
        "1. Definition and Formula\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Measures the probability of misclassifying a randomly chosen sample if it were labeled according to the class distribution.\n",
        "\n",
        "      Gini=1‚àíi=1‚àëc‚Äãpi2‚Äã\n",
        "\n",
        "Entropy\n",
        "\n",
        "Measures the amount of uncertainty or randomness in the data.\n",
        "\n",
        "     Entropy=‚àíi=1‚àëc‚Äãpi‚Äãlog2‚Äã(pi‚Äã)\n",
        "| Aspect                | Gini Impurity                     | Entropy                                    |\n",
        "| --------------------- | --------------------------------- | ------------------------------------------ |\n",
        "| Concept               | Probability of misclassification  | Measure of information/uncertainty         |\n",
        "| Output Range (Binary) | 0 to 0.5                          | 0 to 1                                     |\n",
        "| Computation           | Simpler (no logarithms)           | More complex (uses log)                    |\n",
        "| Sensitivity           | Less sensitive to class imbalance | More sensitive to changes in probabilities |\n",
        "| Bias                  | Favors larger class dominance     | Provides more informative splits           |\n",
        "\n",
        "3. Behavior and Practical Impact\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "-   Faster to compute ‚Üí preferred in CART\n",
        "\n",
        "-   Tends to create more balanced trees\n",
        "\n",
        "-   Performs well when speed is important\n",
        "\n",
        "-   Slightly favors majority classes\n",
        "\n",
        "Entropy\n",
        "\n",
        "-   Used in ID3 and C4.5\n",
        "\n",
        "-   Produces splits with higher information content\n",
        "\n",
        "-   More sensitive to rare classes\n",
        "\n",
        "-   Often results in slightly deeper trees\n",
        "\n",
        "4. When to Use Which?\n",
        "\n",
        "Use Gini Impurity when:\n",
        "\n",
        "-   You want faster training\n",
        "\n",
        "-   Working with large datasets\n",
        "\n",
        "-   Slight performance differences don‚Äôt matter much\n",
        "\n",
        "Use Entropy when:\n",
        "\n",
        "-   Interpretability of splits is important\n",
        "\n",
        "-   You care about information-theoretic meaning\n",
        "\n",
        "-   The dataset has imbalanced classes\n",
        "\n",
        "5. Similarities\n",
        "\n",
        "-   Both measure node impurity\n",
        "\n",
        "-   Both are minimum (0) when node is pure\n",
        "\n",
        "-   Both are used to choose the best split\n",
        "\n",
        "-   Often produce very similar trees in practice\n",
        "\n",
        "6. Summary\n",
        "\n",
        "-   Gini Impurity is faster and simpler\n",
        "\n",
        "-   Entropy is more theoretically informative\n",
        "\n",
        "-   Choice usually has minimal impact on accuracy\n",
        "\n",
        "-   Decision depends on performance needs vs interpretability\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer- **Pre-Pruning in Decision Trees** is a technique used to **stop the tree from growing too large during training**. Instead of allowing the tree to split until every leaf is perfectly pure, pre-pruning applies certain **stopping rules** to prevent overfitting.\n",
        "\n",
        "It works by **halting further splits** when a condition is met, such as:\n",
        "\n",
        "* The maximum depth of the tree is reached\n",
        "* The number of samples in a node is too small\n",
        "* The improvement in impurity (Information Gain or Gini reduction) is below a threshold\n",
        "\n",
        "The main purpose of pre-pruning is to **improve generalization**, reduce model complexity, and make the tree easier to interpret. However, if applied too aggressively, it can cause **underfitting** by stopping the tree too early.\n",
        "\n",
        "In short, **pre-pruning controls tree growth early to avoid overfitting**.\n",
        "\n",
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer-  This is a simple Python program that trains a Decision Tree Classifier using Gini Impurity and prints the feature importances.\n",
        "\n",
        "(The code uses a small built-in dataset for demonstration.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mC49mniQqP2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8np2JqZ0OqM",
        "outputId": "3e814bdd-dc42-40af-8b64-4f251e376391"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation (Brief):\n",
        "\n",
        "-  criterion='gini' tells the model to use Gini Impurity\n",
        "\n",
        "-   .feature_importances_ shows how important each feature is in making decisions\n",
        "\n",
        "-   Higher value ‚Üí more important feature in the tree\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer-  A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its main goal is to find the best decision boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "In SVM:\n",
        "\n",
        "-   The hyperplane is the line (in 2D), plane (in 3D), or higher-dimensional boundary that separates classes.\n",
        "\n",
        "-  The data points closest to this boundary are called support vectors.\n",
        "\n",
        "-  These support vectors are critical because they define the position of the hyperplane.\n",
        "\n",
        "SVM can handle:\n",
        "\n",
        "-  Linearly separable data using a straight hyperplane\n",
        "\n",
        "-  Non-linearly separable data using kernel functions (such as linear, polynomial, and RBF kernels), which map data into higher dimensions where separation is possible\n",
        "\n",
        "Key Advantages:\n",
        "\n",
        "-  Effective in high-dimensional spaces\n",
        "\n",
        "-   Works well with small and medium-sized datasets\n",
        "\n",
        "-  Robust to overfitting when properly regularized\n",
        "\n",
        "Key Limitation:\n",
        "\n",
        "-   Can be computationally expensive for very large datasets\n",
        "\n",
        "-   Choice of kernel and parameters is important\n",
        "\n",
        "In short:\n",
        "\n",
        "An SVM finds the optimal boundary that maximizes the margin between classes, making it a powerful and accurate classification algorithm.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer- The **Kernel Trick** in **Support Vector Machines (SVM)** is a technique that allows SVMs to **handle non-linearly separable data** without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "Instead of computing the transformation directly, the kernel trick computes the **inner product** of data points in a higher-dimensional feature space **implicitly**, which makes the computation efficient.\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "* Some datasets cannot be separated by a straight line in their original space.\n",
        "* The kernel trick applies a **kernel function** that maps data into a higher-dimensional space.\n",
        "* In this new space, a **linear hyperplane** can separate the data.\n",
        "* The mapping is done **implicitly**, avoiding expensive computations.\n",
        "\n",
        "### Common Kernel Functions:\n",
        "\n",
        "* **Linear Kernel** ‚Äì for linearly separable data\n",
        "* **Polynomial Kernel** ‚Äì captures polynomial relationships\n",
        "* **Radial Basis Function (RBF/Gaussian)** ‚Äì handles complex, non-linear boundaries\n",
        "* **Sigmoid Kernel** ‚Äì similar to neural networks\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "* Enables SVMs to model complex, non-linear patterns\n",
        "* Computationally efficient compared to explicit feature mapping\n",
        "* Increases flexibility of SVM models\n",
        "\n",
        "### In Short:\n",
        "\n",
        "The **kernel trick** allows SVMs to draw **non-linear decision boundaries** by implicitly mapping data into higher dimensions, making complex classification problems solvable efficiently.\n",
        "\n",
        "\n",
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer-  Below is a simple Python program that trains two SVM classifiers (Linear and RBF kernels) on the Wine dataset and compares their accuracies."
      ],
      "metadata": {
        "id": "py0s6vdC0ajL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy with Linear Kernel SVM:\", linear_accuracy)\n",
        "print(\"Accuracy with RBF Kernel SVM:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMXf_NSy3JUx",
        "outputId": "9b259ab7-8bba-4515-b667-221e74998343"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel SVM: 0.9814814814814815\n",
            "Accuracy with RBF Kernel SVM: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion (Brief):\n",
        "\n",
        "-  Both SVM models perform well on the Wine dataset.\n",
        "\n",
        "-  The Linear kernel slightly outperforms the RBF kernel here.\n",
        "\n",
        "-  Performance may vary depending on data split and parameter tuning.\n",
        "\n",
        "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Answer-  The **Na√Øve Bayes classifier** is a **supervised machine learning algorithm** based on **Bayes‚Äô Theorem** and is mainly used for **classification tasks**.\n",
        "\n",
        "It calculates the probability that a data point belongs to a particular class given its features, and then assigns the class with the **highest posterior probability**.\n",
        "\n",
        "It is called **‚ÄúNa√Øve‚Äù** because it makes a **strong simplifying assumption** that **all features are independent of each other**, given the class label.\n",
        "In real-world data, this assumption is usually not true, which is why the model is considered ‚Äúna√Øve.‚Äù\n",
        "\n",
        "### Why Na√Øve Bayes Works Well:\n",
        "\n",
        "* Simple and fast to train\n",
        "* Performs well on high-dimensional data\n",
        "* Effective for text classification (spam detection, sentiment analysis)\n",
        "\n",
        "### Common Types of Na√Øve Bayes:\n",
        "\n",
        "* **Gaussian Na√Øve Bayes** ‚Äì for continuous features\n",
        "* **Multinomial Na√Øve Bayes** ‚Äì for text and count data\n",
        "* **Bernoulli Na√Øve Bayes** ‚Äì for binary features\n",
        "\n",
        "### In Short:\n",
        "\n",
        "Na√Øve Bayes is a probabilistic classifier based on Bayes‚Äô theorem, and it is called ‚Äúna√Øve‚Äù because it assumes feature independence, even though this assumption is often unrealistic.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "Answer-  The **Na√Øve Bayes family** has different variants based on the **type of data** they are designed to handle. The key difference between **Gaussian**, **Multinomial**, and **Bernoulli Na√Øve Bayes** lies in the **assumed distribution of the features**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Gaussian Na√Øve Bayes\n",
        "\n",
        "* Assumes that features follow a **normal (Gaussian) distribution**\n",
        "* Used for **continuous numerical data**\n",
        "* Common in problems like medical measurements or sensor data\n",
        "\n",
        "**Example:** height, weight, temperature\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Multinomial Na√Øve Bayes\n",
        "\n",
        "* Assumes features represent **counts or frequencies**\n",
        "* Mainly used for **text classification**\n",
        "* Works well with **word counts or TF-IDF features**\n",
        "\n",
        "**Example:** number of times a word appears in a document\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "* Assumes features are **binary (0 or 1)**\n",
        "* Focuses on **presence or absence** of features\n",
        "* Often used in text classification with binary word features\n",
        "\n",
        "**Example:** word present (1) or not present (0)\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences Summary\n",
        "\n",
        "| Aspect              | Gaussian NB    | Multinomial NB     | Bernoulli NB                 |\n",
        "| ------------------- | -------------- | ------------------ | ---------------------------- |\n",
        "| Feature Type        | Continuous     | Count-based        | Binary                       |\n",
        "| Data Distribution   | Normal         | Multinomial        | Bernoulli                    |\n",
        "| Typical Use Case    | Numerical data | Text data (counts) | Text data (presence/absence) |\n",
        "| Handles Zero Values | Naturally      | Needs smoothing    | Explicitly models zeros      |\n",
        "\n",
        "---\n",
        "\n",
        "### In Short:\n",
        "\n",
        "* **Gaussian NB** ‚Üí continuous features\n",
        "* **Multinomial NB** ‚Üí word counts / frequencies\n",
        "* **Bernoulli NB** ‚Üí binary features\n",
        "\n",
        "Each variant is chosen based on the **nature of the dataset** being used.\n",
        "\n",
        "Question 10: Breast Cancer Dataset\n",
        "\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer-  Here‚Äôs a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate its accuracy:"
      ],
      "metadata": {
        "id": "6eppgTzt3Nld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naive Bayes on Breast Cancer dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMk7bbeh5APC",
        "outputId": "d4f391a0-5995-494c-935d-9cb3e00c4c3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "-  GaussianNB() is used because the dataset has continuous features.\n",
        "\n",
        "-   train_test_split splits the data into 70% training and 30% testing.\n",
        "\n",
        "-   accuracy_score evaluates the classifier performance on unseen test data.\n",
        "\n",
        "This shows that Gaussian Na√Øve Bayes works well for the Breast Cancer dataset."
      ],
      "metadata": {
        "id": "Ggt-dGE25DgD"
      }
    }
  ]
}